# -*- coding: utf-8 -*-
"""COS732_ISAT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TQsSbocOFRsgLWDgAr265S7xPhVhtRaN
"""

import numpy as np    # linear algebra
import pandas as pd   # data processing

# Machine Learning + Models
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn import metrics
from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay, precision_recall_fscore_support)

# Graph Visualization
import matplotlib.pyplot as plt

# System
import os
mydir = r'/content/Face_Recognition'
myfile = 'Face_Recognition'
Face_Recognition_path = os.path.join(mydir, myfile)

# Loading the image data
data = np.load("/content/Face_Recognition/olivetti_faces.npy")
target = np.load("/content/Face_Recognition/olivetti_faces_target.npy")

# Verifying the abovementioned loaded data
print("There are {} images in the dataset".format(len(data)))
print("There are {} unique targets in the dataset".format(len(np.unique(target))))
print("The size of each image is: {}x{}".format(data.shape[1],data.shape[2]))

# Number of people faces in the dataset (40)
print("Unique Target Number:",np.unique(target))

def show_40_distinct_people(images, unique_ids):
    #Creating 4X10 subplots in  18x9 figure size
    fig, axarr = plt.subplots(nrows=4, ncols=10, figsize=(18, 9))
    
    #For easy iteration flattened 4X10 subplots matrix to 40 array
    #xarr=axarr.flatten()
    
    # Iterating over user ids
    for unique_id in unique_ids:
        image_index=unique_id*10
        axarr[unique_id].imshow(images[image_index], cmap='gray')
        axarr[unique_id].set_xticks([])
        axarr[unique_id].set_yticks([])
        axarr[unique_id].set_title("face id:{}".format(unique_id))
    plt.suptitle("There are 40 distinct people in the dataset")

#show_40_distinct_people(data, np.unique(target))

'''
Showing 10 faces of distinct number of subjects
Each subject has 10 distinct face images
'''

def show_10_faces_of_n_subject(images, subject_ids):
    cols = 10
    rows = (len(subject_ids)*10)/cols 
    rows = int(rows)
    
    fig, axarr = plt.subplots(nrows = rows, ncols = cols, figsize = (18,9))
    
    for i, subject_id in enumerate(subject_ids):
        for j in range(cols):
            image_index = subject_id*10 + j
            axarr[i,j].imshow(images[image_index], cmap = "gray")
            axarr[i,j].set_xticks([])
            axarr[i,j].set_yticks([])
            axarr[i,j].set_title("face id:{}".format(subject_id))

# Can play around with 'subject ids' to see other peoples faces in dataset [0-39]
show_10_faces_of_n_subject(images = data, subject_ids = [0, 4, 7, 14, 17])

# We reshape images for ML models
X = data.reshape((data.shape[0],data.shape[1]*data.shape[2]))
print("X shape:",X.shape)

# Splitting the data and target data into different training and test sets
X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.3, stratify=target, random_state=0)
print("X_train shape:",X_train.shape)
print("y_train shape:{}".format(y_train.shape))

# Number of samples for each class
y_frame = pd.DataFrame()
y_frame['Subject IDs'] = y_train
y_frame.groupby(['Subject IDs']).size().plot.bar(figsize = (15,8),title = "Number of Samples for Each Classes")

!pip install mglearn
import mglearn
mglearn.plots.plot_pca_illustration()

'''
PCA / principle component analysis enables the data to be presented
in a smaller size transforming the new components and size to be reduced
selecting most important components
'''

from sklearn.decomposition import PCA

# 2 components
pca = PCA(n_components=2)
pca.fit(X)
X_pca = pca.transform(X)

number_of_people = 10 
index_range = number_of_people*10 
'''
Changing the number of people [10] thus changes the graph illustration
It is ideal to ensure that the index_range == number_of_people
'''
fig = plt.figure(figsize = (10,8))
ax = fig.add_subplot(1,1,1)
scatter=ax.scatter(X_pca[:index_range,0],
            X_pca[:index_range,1], 
            c=target[:index_range],
            s=10,
           cmap=plt.get_cmap('jet', number_of_people)
          )

ax.set_xlabel("First Principle Component")
ax.set_ylabel("Second Principle Component")
ax.set_title("PCA projection of {} people".format(number_of_people))

fig.colorbar(scatter)

# Finding the optimum number of principle components
pca = PCA()
pca.fit(X)

plt.figure(1, figsize=(12,8))

plt.plot(pca.explained_variance_, linewidth=2)
 
plt.xlabel('Components')
plt.ylabel('Explained Variaces')
plt.show()

# From about 90 PCA components it represents the same data
n_components = 90

# Classifying images using PCA components
pca = PCA(copy=True, iterated_power='auto', n_components = 90, random_state=None, svd_solver='auto', tol=0.0, whiten=True)
pca.fit(X_train)

# Average face
fig,ax = plt.subplots(1,1,figsize = (8,8))
ax.imshow(pca.mean_.reshape((64,64)), cmap = "gray")
ax.set_xticks([])
ax.set_yticks([])
ax.set_title('Average Face')

'''
eigen / characteristics
Combining PCA components to show most characterized faces
'''
number_of_eigenfaces = len(pca.components_)
eigen_faces = pca.components_.reshape((number_of_eigenfaces, data.shape[1], data.shape[2]))

cols = 10
rows = int(number_of_eigenfaces/cols)
fig, axarr = plt.subplots(nrows = rows, ncols=cols, figsize = (15,15))
axarr = axarr.flatten()
for i in range(number_of_eigenfaces):
    axarr[i].imshow(eigen_faces[i],cmap = "gray")
    axarr[i].set_xticks([])
    axarr[i].set_yticks([])
    axarr[i].set_title("eigen id:{}".format(i))
plt.suptitle("All Eigen Faces".format(10*"=", 10*"="))

# Result of classification
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

''' Accuracy scores of models with PCA incorporated '''

# Prints accuracy score of LR
clf = LogisticRegression()
clf.fit(X_train_pca, y_train)
y_pred = clf.predict(X_test_pca)
print("Accuracy score of Logistic Regression: {:.2f}".format(metrics.accuracy_score(y_test, y_pred)))

# Prints accuracy score of LDA
clf = LinearDiscriminantAnalysis()
clf.fit(X_train_pca, y_train)
y_pred = clf.predict(X_test_pca)
print("Accuracy score of Linear Discriminant Analysis: {:.2f}".format(metrics.accuracy_score(y_test, y_pred)))    # inserted score to x by 100

# Prints accuracy score of NB
clf = GaussianNB()
clf.fit(X_train_pca, y_train)
y_pred = clf.predict(X_test_pca)
print("Accuracy score of Gaussian NB: {:.2f}".format(metrics.accuracy_score(y_test, y_pred)))

# Heatmap of a confusion matrix of classification
import seaborn as sns
plt.figure(1, figsize = (12,8))
sns.heatmap(metrics.confusion_matrix(y_test, y_pred))

# Results obtained from LDA, LR, NB
print(metrics.classification_report(y_test, y_pred))

models = []
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(("LR",LogisticRegression()))
models.append(("NB",GaussianNB()))

for name, model in models:
    
    clf = model

    clf.fit(X_train_pca, y_train)

    y_pred = clf.predict(X_test_pca)
    print(10*"=","{} Result".format(name).upper(),10*"=")
    print("Accuracy score:{:0.2f}".format(metrics.accuracy_score(y_test, y_pred)))
    print()

"""LDA and LR have best performance as seen from above"""

# Cross-validation of LDA and LR and NB for best performance
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

pca = PCA(n_components=n_components, whiten=True)
pca.fit(X)
X_pca = pca.transform(X)

for name, model in models:
    kfold=KFold(n_splits=4, shuffle=True, random_state=0)   # 4 Splits (can change)
    
    cv_scores = cross_val_score(model, X_pca, target, cv=kfold)
    print("{} mean cross validations score:{:.2f}".format(name, cv_scores.mean()))

# Learning Rate of the different models
lr = LinearDiscriminantAnalysis()
lr.fit(X_train_pca, y_train)
y_pred = lr.predict(X_test_pca)
print("Accuracy score of LDA: {: .2f}".format(metrics.accuracy_score(y_test, y_pred)))

lr = LogisticRegression()
lr.fit(X_train_pca, y_train)
y_pred = lr.predict(X_test_pca)
print("Accuracy score of LR: {: .2f}".format(metrics.accuracy_score(y_test, y_pred)))

lr = GaussianNB()
lr.fit(X_train_pca, y_train)
y_pred = lr.predict(X_test_pca)
print("Accuracy score of NB: {: .2f}".format(metrics.accuracy_score(y_test, y_pred)))

# Average precision score
import numpy as np
from sklearn.metrics import average_precision_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
print("Average precision score",average_precision_score(y_true, y_scores))



"""3 Architectural models: LR, LDA, NB

Federated Learning environment setup
"""

import numpy as np
import random
import cv2
import os
from imutils import paths
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import backend as K

def load(paths, verbose=-1):

    data = np.load("/content/Face_Recognition/olivetti_faces.npy")
    target = np.load("/content/Face_Recognition/olivetti_faces_target.npy")
    labels = list()
    
    # loop over the input images
    for (i, imgpath) in enumerate(paths):

        # load the image and extract the class labels
        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE) # Reading data as greyscale then flattening it
        image = np.array(im_gray).flatten()
        label = imgpath.split(os.path.sep)[-2]

        # scale the image to [0, 1] and add to list
        data.append(image/255)
        labels.append(label)

        # show an update every `verbose` images
        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:
            print("[INFO] processed {}/{}".format(i + 1, len(paths)))

    # return a tuple of the data and labels
    return data, labels

print("unique target number:",np.unique(target))

# Creating the testing/training splits

# Path to your dataset folder
img_path = "/content/Face_Recognition/"

# Getting the path list using the path object
image_paths = list(paths.list_images(img_path))

# Applying function
image_list, label_list = load(image_paths, verbose=10000)

# We reshape images for ML models
X = data.reshape((data.shape[0],data.shape[1]*data.shape[2]))
print("X shape:",X.shape)

# Splitting the data and target data into different training and test sets
X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.3, stratify=target, random_state=0)
print("X_train shape:",X_train.shape)
print("y_train shape:{}".format(y_train.shape))

# Federated Clients 
def create_clients(image_list, label_list, num_clients=10, initial='clients'):
  
    ''' return: a dictionary with keys clients' names and value as 
                data shards - tuple of images and label lists.
        args: 
            image_list: a list of numpy arrays of training images
            num_client: number of federated members (clients)
            initials: the clients'name prefix, e.g, clients_1

        Training set is shared into 10 shards, 1 per client (10)
    '''

    # Create a list for client names
    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]

    # Randomize the data
    data = list(zip(image_list, label_list))
    random.shuffle(data)

    # Shard data and place at each client
    size = len(data)//num_clients
    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]

    # Number of clients must equal number of shards
    assert(len(shards) == len(client_names))

    # Dictionary containing each client name as a key
    return {client_names[i] : shards[i] for i in range(len(client_names))}

# Creating the federated clients
clients = create_clients(X_train, y_train, num_clients=10, initial='client')

''' Processing and batching the federated clients' and testing data
    into TF dataset then batching them
'''

def batch_data(data_shard, bs=32):

    ''' Takes in a clients data shard and create a tensorflow dataset object off it
    args:
        shard: a data, label constituting a client's data shard
        bs:batch size
    return:
        tfds object
    '''

    # Seperating shard into data and labels lists
    data, label = zip(*data_shard)
    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
    return dataset.shuffle(len(label)).batch(bs)

''' Preparing each client for training and
    process and batch the training data for each client
'''
    
clients_batched = dict()
for (client_name, data) in clients.items():
    clients_batched[client_name] = batch_data(data)
    
# Process and batch the test set  
test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))

# Creating Multi Layer Perceptron Architecture Model (MLP)

''' The MLP serves as the model for classification task '''

class SimpleMLP:
    @staticmethod
    def build(shape, classes):
        model = Sequential()
        model.add(Dense(128, input_dim=shape))
        model.add(Activation("relu"))
        model.add(Dense(128))
        model.add(Activation("relu")) 
        model.add(Dense(classes))
        model.add(Activation("softmax")) 
        return model

# Declaring the Loss function, Metric and Optimizer to compile models
learning_rate = 0.01 
comms_round = 100
loss = 'categorical_crossentropy'
metrics = ['accuracy']
optimizer = SGD(learning_rate=learning_rate, 
                decay = learning_rate / comms_round,   # Comms_round is the number of local epochs
                momentum = 0.9
               )

# Implementing FedAvg

''' Claculating the clients local training data with overall training data held
    by all of the clients.
    Obtain client batch size to calculate number of data points.
    Calculate the scaling factor as a fraction specified in line 18
'''

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())

    # Get the batch size
    bs = list(clients_trn_data[client_name])[0][0].shape[0]

    # First calculate the total training data points across clients
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs

    # Get the total number of data points held by a client
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count

def scale_model_weights(weight, scalar):
    '''function for scaling a models weights based on value of ther scaling factor above'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final

def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()

    #Get the average gradient accross all client gradients
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=1)
        avg_grad.append(layer_mean)
        
    return avg_grad

def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

    Logits = model.predict(X_test, batch_size=100)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits), tf.argmax(Y_test))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

# FL Training 
# Initialize global m
from tqdm.notebook import tqdm

smlp_global = SimpleMLP()
global_model = smlp_global.build(4096, 1)   
        
# Commence global training loop
for comm_round in range(comms_round):
            
    # Get the global model's weights - will serve as the initial weights for all local models
    global_weights = global_model.get_weights()
    
    # Initial list to collect local model weights after scalling
    scaled_local_weight_list = list()

    ''' Randomizing client data using keys to ensure randomness
        followed by interating through client training'''
    client_names = list(clients_batched.keys())
    random.shuffle(client_names)
    
    # Loop through each client and create new local model
    for client in tqdm(client_names):
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(4096, 1)
        local_model.compile(loss = loss, 
                      optimizer = optimizer, 
                      metrics = metrics)
        
        ''' New model object is created for each client which is then compiled
            and initialisation weights to the current parameters of the global
            model.
            Local model (client) is trained for 1 epoch followed by the new
            weights that were scaled and appeneded to scaled_local_weight_list
        '''
        
        # Set local model weight to the weight of the global model
        local_model.set_weights(global_weights)
        
        # Fit local model with client's data
        local_model.fit(clients_batched[client], epochs=1, batch_size=2)   # epochs orignally 1
        
        # Scale the model weights and add to list
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights) # Local training
        
        # Clear session to free memory after each communication round
        K.clear_session()
        
    # To get the average over all the local model, we simply take the sum of the scaled weights
    average_weights = sum_scaled_weights(scaled_local_weight_list)

    # Update global model 
    global_model.set_weights(average_weights)

    # Test global model and print out metrics after each communications round
    for(X_test, Y_test) in tqdm(test_batched):
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)

# FL Testing
def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

    Logits = model.predict(X_test, batch_size=100)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss
